<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" slick-uniqueid="3">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="keywords" content="Xin-Yang Zheng, Tsinghua University, 郑欣阳, 清华大学, 3D Generation">
<meta name="description" content="Locally Attentional SDF Diffusion for Controllable 3D Shape Generation">
<link rel="stylesheet" href="../homepage_files/style/jemdoc.css" type="text/css">
<style type="text/css">
</style>
<title>LAS-Diffusion</title>

</head>

<body>
<div id="layout-content" style="margin-top:25px">

<!-- teaser -->
<table border="0" width="100%"> <tbody>
<tr>
  <td valign="top" align="center">
    <b><font size="5" face="Times New Roman" >Locally Attentional SDF Diffusion for Controllable 3D Shape Generation</font></b>
    <br><br>
  </td>
</tr>

<tr>
  <td valign="top" align="center">
    <font size="4" face="Times New Roman" > <a href="https://zhengxinyang.github.io/"  target="_blank">Xin-Yang Zheng</a> </font> <font size="2"><sup>1</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://haopan.github.io/"  target="_blank">Hao Pan</a> </font> <font size="2"><sup>2</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://wang-ps.github.io/" target="_blank" >Peng-Shuai Wang</a> </font> <font size="2"><sup>3</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="http://research.microsoft.com/en-us/um/people/xtong/xtong.html"  target="_blank">Xin Tong</a>  </font> <font size="2"><sup>2</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://xueyuhanlang.github.io/"  target="_blank">Yang Liu</a> </font> <font size="2"><sup>2</sup></font> &emsp;&emsp;
    <font size="4" face="Times New Roman" > <a href="https://www.microsoft.com/en-us/research/people/hshum/"  target="_blank">Heung-Yeung Shum</a> </font> <font size="2"><sup>1, 4</sup></font> 
    <br> 
    <font size="2"> <sup>1</sup></font> <font size="3" face="Times New Roman" > <a href="https://www.tsinghua.edu.cn/en/index.htm" target="_blank">Tsinghua University</a></font>
    &emsp;&emsp;
    <font size="2"> <sup>2</sup></font> <font size="3" face="Times New Roman" ><a href="http://research.microsoft.com/en-us/labs/asia/" target="_blank">Microsoft Research Asia</a> </font>
    &emsp;&emsp;
    <font size="2"> <sup>3</sup></font> <font size="3" face="Times New Roman" > <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a></font>
    &emsp;&emsp;
    <font size="2"> <sup>4</sup></font> <font size="3" face="Times New Roman" ><a href="https://www.idea.edu.cn/" target="_blank">International Digital Economy Academy</a> </font>
    &emsp;&emsp;
    <br>
    <font size="3" face="Times New Roman" > ACM Transactions on Graphics (SIGGRAPH 2023) </font>
    <br> <br>
  </td>
</tr>

<tr>
  <td valign="top" align="center">
      <img width="90%" src="./LASDiffusion_files/representative_full.jpg">
  </td>
</tr>
</tbody> </table>

<!-- Abstract -->
<table border="0" width="100%"> <tbody>
<h2>Abstract</h2>
<tr>
  <p style="text-align:justify;">
  <font face="Times New Roman" >
    Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework --- <em>locally attentional SDF diffusion</em>, to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named <em>occupancy-diffusion</em>, aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named <em>SDF-diffusion</em>, synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel <em>view-aware local attention</em> mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work.
  </font>
  </p>
</tr>
<!-- Demo -->
<table border="0" width="100%"> <tbody>
  <h2>Demo</h2>
  <tr>
    <center>
      <video width="80%" height="80%" controls>
        <source src="./LASDiffusion_files/project_page.mp4" type="video/mp4">
      </video>
    </center>
  </tr>
<tr>


  <table border="0" width="100%"> <tbody>
    <h2>Links</h2>
  <td align="center" width="25%">
    <img src="./LASDiffusion_files/representative.jpg" width="200">
  </td>
  <td>
    <!-- todo: modify arxiv version -->
    <strong>Paper</strong> [<a href="https://arxiv.org/abs/2305.04461" target="_blank">PDF</a>] <br><br>
   
    <strong>Code</strong> [<a href="https://github.com/Zhengxinyang/LAS-Diffusion"  target="_blank">Github</a>] <br><br>

    <strong>Supplemental</strong> [<a href="https://drive.google.com/file/d/17THBCQ8_j8Zft2mycxh4iWAqYSb3qfCT/view?usp=sharing" target="_blank">ZIP</a>]<br><br>
     
    <strong>Citation</strong> [<a href="./LASDiffusion_files/ref.bib" target="_blank">BibTeX</a>]<br>
  </td>
</tr>
</tbody> </table>
</div>

</body>
</html>